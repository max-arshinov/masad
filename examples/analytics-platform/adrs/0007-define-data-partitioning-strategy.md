# 7. Define Data Partitioning Strategy

Date: 2025-09-03

## Status

Proposed

## Context

Iteration goal: Define partitioning strategy for Amazon MSK to distribute 1.16M events/sec sustained load across brokers and enable horizontal scaling to 146B events/day by year 5. Must ensure even load distribution while maintaining event ordering where required.

Business drivers: Analytics platform tracks events from multiple websites with varying traffic patterns. Partition strategy affects query performance, consumer parallelism, and operational complexity.

Relevant QAs (IDs): P-2 Performance (High event ingestion throughput), S-1 Scalability (Event volume growth), R-1 Reliability (Data durability), A-2 Availability (System uptime).

## Decision

### Compare **Partitioning Strategies**

| Strategy             | Load Distribution    | Event Ordering | Hotspot Risk     | Query Performance     | Operational Complexity |
|----------------------|----------------------|----------------|------------------|-----------------------|------------------------|
| **Website ID**       | ğŸŸ¨ uneven by size    | ğŸŸ© per website | ğŸŸ¥ popular sites | ğŸŸ© site-based queries | ğŸŸ© simple              |
| **User ID Hash**     | ğŸŸ© even distribution | ğŸŸ© per user    | ğŸŸ¨ active users  | ğŸŸ¨ cross-user queries | ğŸŸ¨ moderate            |
| **Timestamp Bucket** | ğŸŸ© time-based        | ğŸŸ¥ none        | ğŸŸ¨ peak hours    | ğŸŸ© time-range queries | ğŸŸ© simple              |
| **Composite Key**    | ğŸŸ© balanced          | ğŸŸ¨ partial     | ğŸŸ© distributed   | ğŸŸ¨ complex routing    | ğŸŸ¥ complex             |
| **Random Hash**      | ğŸŒŸ perfectly even    | ğŸŸ¥ no ordering | ğŸŸ© no hotspots   | ğŸŸ¥ poor locality      | ğŸŸ© simple              |

### Compare **Partition Count Strategies**

| Partition Count        | Throughput Scaling | Consumer Scaling     | Storage Overhead   | Rebalancing Impact | Broker Utilization |
|------------------------|--------------------|----------------------|--------------------|--------------------|--------------------| 
| **Low (100-500)**      | ğŸŸ¥ limited         | ğŸŸ¥ max 500 consumers | ğŸŸ© minimal         | ğŸŸ© fast rebalance  | ğŸŸ¨ uneven          |
| **Medium (1000-2000)** | ğŸŸ© good scaling    | ğŸŸ© good parallelism  | ğŸŸ© reasonable      | ğŸŸ¨ moderate        | ğŸŸ© balanced        |
| **High (5000+)**       | ğŸŒŸ excellent       | ğŸŒŸ high parallelism  | ğŸŸ¨ higher overhead | ğŸŸ¥ slow rebalance  | ğŸŸ© even            |
| **Dynamic Scaling**    | ğŸŒŸ adaptive        | ğŸŒŸ flexible          | ğŸŸ¨ variable        | ğŸŸ¥ complex mgmt    | ğŸŸ© optimized       |

**Decision:** Implement **composite key partitioning** using `hash(website_id + timestamp_hour)` with **2000 partitions** initially. This strategy balances load distribution across websites while providing time-based locality for analytics queries. The composite approach prevents hotspots from high-traffic websites while maintaining some temporal ordering for processing efficiency.

Partition scaling plan: Start with 2000 partitions, scale to 5000 by year 3 when approaching 1.4M events/sec sustained load.

Supersedes: none.

## Consequences

- âœ… Even load distribution prevents broker hotspots during traffic spikes (P-2, A-2).
- âœ… Supports horizontal scaling to 2000+ parallel consumers (S-1).
- âœ… Time-based locality improves analytics query performance and consumer efficiency.
- âœ… Website-based distribution maintains some ordering for per-site processing.
- âœ… 2000 partitions provide headroom for growth while maintaining manageable rebalancing.
- âš ï¸ Composite key increases partitioning logic complexity in producers.
- âš ï¸ Requires monitoring for partition skew and rebalancing when scaling.
- Follow-ups: ADR-008 (auto-scaling policies), producer partition key implementation, consumer group rebalancing strategies.
