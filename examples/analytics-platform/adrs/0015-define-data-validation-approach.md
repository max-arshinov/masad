# 15. Define Data Validation Approach

Date: 2025-09-03

## Status

Proposed

## Context

Iteration goal: Define comprehensive data validation approach across stream and batch processing layers to ensure zero data loss and maintain data quality while processing 1.16M events/sec with 50-100min processing latency SLA.

Business drivers: Analytics platform must detect and handle data corruption, schema violations, and processing errors without impacting downstream analytics. Validation must scale with event volume growth while providing audit trails for compliance.

Relevant QAs (IDs): R-1 Reliability (Zero data loss), A-1 Availability (Data processing latency within 50-100min), SE-1 Security (Data privacy compliance), R-2 Reliability (Query consistency).

## Decision

### Compare **Validation Strategies**

| Strategy              | Error Detection      | Performance Impact  | Recovery Capability| Audit Trail        | Operational Overhead| Data Quality Assurance|
|-----------------------|----------------------|---------------------|--------------------|--------------------|---------------------|----------------------|
| **Schema Validation** | ğŸŸ© structure errors | ğŸŸ© minimal overhead| ğŸŸ¨ reject/retry    | ğŸŸ© logged events   | ğŸŸ© automated       | ğŸŸ© prevents corruption|
| **Business Rules**    | ğŸŸ© logic violations | ğŸŸ¨ rule complexity  | ğŸŸ© configurable   | ğŸŸ© detailed logs   | ğŸŸ¨ rule maintenance | ğŸŒŸ domain accuracy   |
| **Statistical Anomaly**| ğŸŸ© pattern deviations| ğŸŸ¨ computation cost| ğŸŸ¨ manual review   | ğŸŸ© trend analysis  | ğŸŸ¨ model tuning     | ğŸŸ© detects drift     |
| **End-to-End Checksums**| ğŸŒŸ data integrity | ğŸŸ¨ hash computation | ğŸŸ© replay capability| ğŸŒŸ complete audit | ğŸŸ¨ infrastructure   | ğŸŒŸ guarantees accuracy|

### Compare **Validation Technologies**

| Technology            | Real-time Validation | Batch Validation    | Schema Evolution   | Error Handling     | Monitoring         | Integration Effort |
|-----------------------|----------------------|---------------------|--------------------|--------------------|--------------------|--------------------|
| **Apache Avro + Registry**| ğŸŸ© schema checks | ğŸŸ© backward compat | ğŸŒŸ evolution support| ğŸŸ¨ fail-fast      | ğŸŸ¨ basic metrics   | ğŸŸ© Kafka native    |
| **Great Expectations** | ğŸŸ¨ lightweight     | ğŸŒŸ comprehensive   | ğŸŸ¨ manual updates  | ğŸŸ© flexible actions| ğŸŸ© rich reporting  | ğŸŸ¨ Python ecosystem|
| **AWS Glue Data Quality**| ğŸŸ© real-time      | ğŸŸ© scheduled       | ğŸŸ© managed         | ğŸŸ© configurable   | ğŸŸ© CloudWatch      | ğŸŸ© AWS native      |
| **Custom Kafka Streams**| ğŸŒŸ inline validation| ğŸŸ¨ limited scope  | ğŸŸ¨ manual coding   | ğŸŸ© custom logic    | ğŸŸ¨ custom metrics  | ğŸŸ© existing framework|
| **ClickHouse Constraints**| ğŸŸ¨ insert-time   | ğŸŸ© query-time      | ğŸŸ© SQL-based      | ğŸŸ¨ reject records  | ğŸŸ¨ basic logs      | ğŸŸ© database native |

**Decision:** Implement **multi-layer validation architecture**:

**Layer 1 - Stream Validation (Kafka Streams + Avro):**
- Schema validation using Confluent Schema Registry
- Basic business rules (required fields, value ranges)
- Dead letter queue for invalid events
- <1ms validation latency per event

**Layer 2 - Batch Validation (Great Expectations):**
- Statistical anomaly detection every 15 minutes
- Complex business rule validation
- Data quality metrics and alerting
- Historical trend analysis

**Layer 3 - Storage Validation (ClickHouse):**
- Database constraints for final data integrity
- Duplicate detection and deduplication
- Cross-table consistency checks
- Query-time validation alerts

Supersedes: none.

## Consequences

- âœ… Zero data loss through comprehensive validation at ingestion, processing, and storage (R-1).
- âœ… Sub-minute validation in stream layer maintains 50-100min processing SLA (A-1).
- âœ… Schema evolution support enables backward compatibility and smooth deployments.
- âœ… Audit trail creation supports GDPR/CCPA compliance requirements (SE-1).
- âœ… Statistical monitoring detects data quality issues before they impact analytics (R-2).
- âš ï¸ Multi-layer validation adds operational complexity and monitoring overhead.
- âš ï¸ Performance impact from validation may require optimization under peak loads.
- Follow-ups: ADR-016 (backup procedures), implement validation rule configuration, set up alerting thresholds.
