# 12. Choose Load Distribution and Hotkey Handling

Date: 2025-09-01

## Status

Proposed

## Context

Iteration 3 goal: Fault Tolerance and High Availability. System must handle hotkey concentration and peak traffic distribution while maintaining <0.1s degradation during failures and ensuring 99.9% uptime across varying load patterns.

Business drivers: Viral content handling, peak traffic resilience, consistent user experience, prevention of cascading failures from popular URLs.

Relevant QAs (IDs): P-3 (node failure tolerance), A-1 (99.9% uptime), P-1 (redirect latency <0.2s).

## Decision

### Table 1 â€” Compare Load Distribution Strategies

| Distribution Strategy | Hotkey Resilience | Load Balance | Failure Isolation | Implementation Complexity | Scalability | Cache Efficiency |
|----------------------|------------------|--------------|-------------------|--------------------------|-------------|----------------|
| **Round Robin** | ðŸŸ¥ poor hotkey handling | ðŸŸ© even distribution | ðŸŸ¨ cascading failures | ðŸŸ© simple | ðŸŸ¨ limited | ðŸŸ¨ random caching |
| **Consistent Hashing** | ðŸŸ¨ hotkey concentration | ðŸŸ© stable distribution | ðŸŸ© isolated failures | ðŸŸ¨ hash implementation | ðŸŸ© elastic scaling | ðŸŸ© locality |
| **Weighted Round Robin** | ðŸŸ¨ capacity-based | ðŸŸ© capacity-aware | ðŸŸ¨ weighted failures | ðŸŸ¨ weight management | ðŸŸ¨ static weights | ðŸŸ¨ predictable |
| **Least Connections** | ðŸŸ¨ connection-based | ðŸŸ© adaptive | ðŸŸ© overload protection | ðŸŸ¨ state tracking | ðŸŸ¨ stateful | ðŸŸ¨ connection locality |
| **Random with Power of Two** | ðŸŸ© distributed sampling | ðŸŸ© balanced load | ðŸŸ© failure resilience | ðŸŸ© simple algorithm | ðŸŸ© stateless | ðŸŸ¨ random distribution |
| **Rendezvous Hashing** | ðŸŸ© distributed hotkeys | ðŸŸ© stable mapping | ðŸŸ© node failure tolerance | ðŸŸ¨ computation overhead | ðŸŸ© elastic | ðŸŸ© consistent locality |

**Shortlist:** Consistent Hashing and Rendezvous Hashing best address hotkey distribution with failure tolerance.

### Table 2 â€” Compare Hotkey Mitigation Techniques

| Mitigation Technique | Hotkey Detection | Load Reduction | Implementation | Response Time | Scalability | Cache Hit Rate |
|---------------------|------------------|----------------|----------------|---------------|-------------|----------------|
| **Multi-tier Caching** | ðŸŸ¨ reactive | ðŸŸ© layer distribution | ðŸŸ© standard CDN/Redis | ðŸŸ© cache speeds | ðŸŸ© horizontal | ðŸŒŸ high hit rates |
| **Hotkey Replication** | ðŸŸ© proactive monitoring | ðŸŒŸ dedicated replicas | ðŸŸ¨ dynamic replication | ðŸŸ© dedicated resources | ðŸŸ¨ resource intensive | ðŸŒŸ 100% for hotkeys |
| **Circuit Breaker** | ðŸŸ© failure detection | ðŸŸ¨ fallback only | ðŸŸ© library integration | ðŸŸ¨ degraded mode | ðŸŸ© stateless | ðŸŸ¥ bypasses cache |
| **Rate Limiting** | ðŸŸ¨ threshold-based | ðŸŸ¨ request throttling | ðŸŸ© API gateway | ðŸŸ¥ delayed requests | ðŸŸ© distributed | ðŸŸ¨ reduces load |
| **Shard Splitting** | ðŸŸ© load monitoring | ðŸŒŸ balanced redistribution | ðŸŸ¥ complex resharding | ðŸŸ© restored balance | ðŸŒŸ elastic | ðŸŸ© redistributed |
| **Edge Computing** | ðŸŸ¨ geographic patterns | ðŸŒŸ geographic distribution | ðŸŸ¨ edge deployment | ðŸŒŸ edge latency | ðŸŒŸ global scale | ðŸŸ© edge caching |

**Decision:** Implement Rendezvous Hashing with Multi-tier Caching and Hotkey Replication:

**Load Distribution Architecture:**
- Rendezvous hashing for consistent URL-to-server mapping
- AWS Application Load Balancer with sticky sessions disabled
- Kubernetes horizontal pod autoscaler based on CPU and custom metrics
- Health check-based node exclusion with automatic recovery

**Hotkey Detection and Mitigation:**
- Real-time hotkey detection using Redis counters with sliding windows
- Automatic hotkey replication to dedicated high-capacity cache nodes
- CDN edge caching with 5-minute TTL for viral content
- Circuit breaker pattern for database protection during traffic spikes

**Multi-tier Caching Strategy:**
```
Tier 1: CloudFront CDN (Global Edge) - 5 min TTL
Tier 2: Redis Cluster (Regional) - 60 min TTL  
Tier 3: Application Cache (Local) - 10 min TTL
Tier 4: Database Read Replicas (Fallback)
```

**Hotkey Handling Algorithm:**
1. Monitor request frequency per URL (1-minute sliding window)
2. Identify hotkeys exceeding 1000 requests/minute threshold
3. Replicate hotkey data to 3+ dedicated cache nodes
4. Route hotkey requests using weighted distribution
5. Gradual de-escalation when traffic normalizes

**Failure Tolerance:**
- Rendezvous hashing maintains consistent mapping during node failures
- Automatic cache warming for replacement nodes
- Cross-region cache replication for disaster recovery
- Graceful degradation to database reads with circuit breaker protection

Supersedes: none.

## Consequences

- âœ… Addresses P-3 with <0.1s degradation through hotkey distribution and circuit breakers
- âœ… Maintains A-1 uptime with failure-tolerant load distribution
- âœ… Preserves P-1 latency through multi-tier caching and edge distribution
- âœ… Handles viral content scenarios with automatic hotkey replication
- âš ï¸ Increased monitoring complexity for hotkey detection and cache coordination
- âš ï¸ Additional infrastructure costs for dedicated hotkey cache nodes
- Follow-ups: ADR on cache invalidation strategies, hotkey threshold tuning, cross-region cache synchronization policies
